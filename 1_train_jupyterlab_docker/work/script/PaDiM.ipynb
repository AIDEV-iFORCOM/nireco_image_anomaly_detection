{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7017c035-1de7-4e6c-ac3b-0d71fd54ab5e",
   "metadata": {},
   "source": [
    "# PaDiM Training\n",
    "\n",
    "- PaDiMモデルを単一カテゴリデータ(Normal)で学習します\n",
    "- 推論には 「n_features」 が必要になります"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc0d60a-0984-4cf7-bdc4-fd6be204e5fa",
   "metadata": {},
   "source": [
    "### 0. CUDA Version 確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc559f6-5449-4743-9576-200dd651c668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch, platform\n",
    "print(\"torch version :\", torch.__version__)\n",
    "print(\"cuda in torch :\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a317c868-153a-48ce-a56f-a3a1430d7e3c",
   "metadata": {},
   "source": [
    "## 1. 初期設定\n",
    "\n",
    "- 処理する画像サイズ 「IMAGE_SIZE」 と、検出対象とする画像カテゴリ 「CATEGORY」を指定してください\n",
    "- 作成された「RUN_DIR」に、学習済モデルが作成されます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60b25992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN DIR: /workspace/models/PaDiM/VisA_pipe_fryum/20250430_231424\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from collections import defaultdict\n",
    "import json, yaml, torch, timm, gc, shutil\n",
    "from anomalib.data import Folder\n",
    "from anomalib.models import Padim\n",
    "from anomalib.engine import Engine\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# -------- ユーザ設定項目 --------\n",
    "IMAGE_SIZE  = 256                              # 画像サイズ(★変更対象)\n",
    "IMAGE_THRESHOLD  = 0.5                         # 異常検出閾値(★変更対象)\n",
    "CATEGORY    = \"VisA_pipe_fryum\"                # 検出対象のカテゴリ(★変更対象)\n",
    "DATA_ROOT   = Path(\"/workspace/data\")          # データフォルダ(tarin/test)\n",
    "# -------------------------------\n",
    "\n",
    "JST = timezone(timedelta(hours=9))\n",
    "timestamp = datetime.now(JST).strftime('%Y%m%d_%H%M%S')       # 実行時のシステム日時\n",
    "OUTPUT_DIR  = Path(\"/workspace/models\") / \"PaDiM\" / CATEGORY  # 学習済モデルの出力先\n",
    "RUN_DIR   = OUTPUT_DIR / timestamp\n",
    "(RUN_DIR / 'checkpoint').mkdir(parents=True, exist_ok=True)\n",
    "(RUN_DIR / 'pytorch').mkdir(parents=True, exist_ok=True)\n",
    "TEMP_DIR = RUN_DIR / \"temp\"\n",
    "PARAM_DIR = RUN_DIR / \"param\"\n",
    "LOG_DIR = RUN_DIR / \"logs\"\n",
    "\n",
    "print('RUN DIR:', RUN_DIR)\n",
    "\n",
    "# -------- DataModule定義 --------\n",
    "def build_datamodule() -> Folder:\n",
    "    return Folder(\n",
    "        name=CATEGORY,\n",
    "        root=DATA_ROOT,\n",
    "        normal_dir=f\"train/{CATEGORY}\",             # 学習用正常データ\n",
    "        abnormal_dir=f\"test/{CATEGORY}/anomaly\",    # テスト(評価)用異常データ (学習では使われない/Optuna探索では使われる)\n",
    "        normal_test_dir=f\"test/{CATEGORY}/normal\",  # テスト(評価)用正常データ\n",
    "        train_batch_size=16,         # default=32\n",
    "        eval_batch_size=16,          # default=32\n",
    "        extensions=(\n",
    "            \".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\",\n",
    "            \".JPG\", \".JPEG\", \".PNG\", \".BMP\", \".TIF\", \".TIFF\", \".WEBP\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "# -------- n_features_max 自動生成定義 --------\n",
    "def _canon(layer_name: str) -> str:\n",
    "    \"\"\"layer2 → layer2 / blocks.2 → blocks.2 のように正規化\"\"\"\n",
    "    return layer_name.strip()\n",
    "\n",
    "def build_nfeat_table(backbones, layer_opts):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        dict[(backbone, layers_tuple)] -> int  (sum of channels of chosen layers)\n",
    "    \"\"\"\n",
    "    nfeat_tbl = {}\n",
    "    for backbone in backbones:\n",
    "        # timm の特徴抽出器\n",
    "        model = timm.create_model(backbone, features_only=True, pretrained=True)\n",
    "        # {モジュール名: 出力チャネル数} を作る\n",
    "        chan_map = {fi[\"module\"]: fi[\"num_chs\"] for fi in model.feature_info}\n",
    "        for layers in layer_opts:\n",
    "            ch_sum = sum(chan_map[_canon(l)] for l in layers)\n",
    "            nfeat_tbl[(backbone, layers)] = ch_sum      # ← PaDiM の上限値\n",
    "    return nfeat_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd2fc0",
   "metadata": {},
   "source": [
    "## 2. ハイパーパラメータ探索 (Optuna)\n",
    "\n",
    "- ここではハイパーパラメータの自動探索を行います\n",
    "- PaDiM は、教師なし学習ですが、Optuna 探索は検証用に異常ラベルを必要とする教師ありプロセスと言えます\n",
    "- 探索を行わずに手動でハイパーパラメータを調整する場合は、ここをスキップして「3. 学習」へ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78607bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna, torch, yaml, types\n",
    "from anomalib.models import Padim\n",
    "from anomalib.engine import Engine\n",
    "\n",
    "# -------- GPU利用可否 --------\n",
    "GPU_OK = torch.cuda.is_available()\n",
    "\n",
    "# -------- サーチスペース(★変更対象) --------\n",
    "N_TRIALS   = 3                                # 試行回数\n",
    "BACKBONES  = [\"resnet18\", \"wide_resnet50_2\"]  # バックボーンCNN(複数可)\n",
    "LAYER_OPTS = [\n",
    "    (\"layer2\",),\n",
    "    # (\"layer2\", \"layer3\"),\n",
    "    # (\"layer1\", \"layer2\", \"layer3\"),\n",
    "]\n",
    "N_FEAT_MAX = build_nfeat_table(BACKBONES, LAYER_OPTS)\n",
    "\n",
    "# -------- Objective --------\n",
    "def objective(trial: optuna.Trial):\n",
    "\n",
    "    backbone = trial.suggest_categorical(\"backbone\", BACKBONES)\n",
    "\n",
    "    # layers は Tuple → Index にして渡す\n",
    "    idx    = trial.suggest_categorical(\"layers_idx\", list(range(len(LAYER_OPTS))))\n",
    "    layers = LAYER_OPTS[idx]\n",
    "\n",
    "    # n_features 上限値\n",
    "    n_feat = N_FEAT_MAX[(backbone, layers)]\n",
    "\n",
    "    # Data & Model\n",
    "    dm  = build_datamodule()\n",
    "    pre = Padim.configure_pre_processor(image_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "    model = Padim(\n",
    "        backbone      = backbone,\n",
    "        layers        = layers,\n",
    "        n_features    = n_feat,\n",
    "        pre_processor = pre,\n",
    "    )\n",
    "\n",
    "    # Engine (GPU/CPUフォールバック)\n",
    "    used_gpu = None\n",
    "    for use_gpu in (GPU_OK, False):\n",
    "        try:\n",
    "            engine = Engine(\n",
    "                logger=False,\n",
    "                default_root_dir=TEMP_DIR / \"optuna\",\n",
    "                accelerator=\"gpu\" if use_gpu else \"cpu\",\n",
    "                max_epochs=1,      # 1 epoch 評価\n",
    "                enable_progress_bar=False,\n",
    "            )\n",
    "            engine.fit(model=model, datamodule=dm)\n",
    "            used_gpu = use_gpu\n",
    "            break\n",
    "        except RuntimeError as e:\n",
    "            if \"cudaGetDeviceCount\" in str(e):\n",
    "                print(\"⚠️ CUDA 初期化エラー → CPU で再試行\")\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "    trial.set_user_attr(\"used_gpu\", used_gpu)\n",
    "    if not used_gpu:\n",
    "        print(f\"Trial {trial.number}: CPU で実行 (GPU fallback)\")\n",
    "\n",
    "    return engine.trainer.callback_metrics.get(\n",
    "        \"image_AUROC\", torch.tensor(0.0)\n",
    "    ).item()\n",
    "\n",
    "# -------- 探索実行 --------\n",
    "TEMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "print(\"BEST :\", study.best_params)\n",
    "print(\"AUROC:\", study.best_value)\n",
    "\n",
    "# -------- 結果保存 --------\n",
    "PARAM_DIR.mkdir(exist_ok=True)\n",
    "search_space = dict(\n",
    "    backbone   = BACKBONES,\n",
    "    layers     = LAYER_OPTS,\n",
    "    n_features = \"auto (<= channel sum)\",\n",
    ")\n",
    "yaml.safe_dump(search_space, open(PARAM_DIR / \"search_space.yaml\", \"w\"))\n",
    "best = study.best_params.copy()\n",
    "best[\"layers\"]      = LAYER_OPTS[best.pop(\"layers_idx\")]\n",
    "best[\"n_features\"]  = N_FEAT_MAX[(best[\"backbone\"], best[\"layers\"])]\n",
    "yaml.safe_dump(best, open(PARAM_DIR / \"best_params.yaml\", \"w\"))\n",
    "study.trials_dataframe().to_csv(PARAM_DIR / \"trials.csv\", index=False)\n",
    "\n",
    "# ---------- TEMP_DIR のクリーンアップ ----------\n",
    "# Debugする場合は、コメントアウトしてください\n",
    "import shutil, gc\n",
    "if TEMP_DIR.exists():\n",
    "    # Windows でハンドルが残ると削除に失敗することがあるので、念のため GC\n",
    "    gc.collect()\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73415335",
   "metadata": {},
   "source": [
    "## 3. 学習\n",
    "\n",
    "- 「2. ハイパーパラメータ探索(Optuna)」 を行った場合は、保存されたベストパラメータで学習します\n",
    "- 自動探索を行わない行場合は、ハイパーパラメータ 「MANUAL_PARAMS」を手動で調整してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1891f0",
   "metadata": {},
   "outputs": [
   ],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from anomalib.models import Padim\n",
    "from anomalib.engine import Engine\n",
    "from anomalib.deploy import ExportType\n",
    "import numpy as np, torch, yaml, types\n",
    "import logging\n",
    "\n",
    "# -------- GPU利用可否 --------\n",
    "GPU_OK = torch.cuda.is_available()\n",
    "\n",
    "# -------- 学習設定 --------\n",
    "MAX_EPOCHS = 1   # 1回のみ実施(μ・Σの算出) ※固定\n",
    "\n",
    "# -------- ★手動パラメータ（best_params.yaml が存在しない場合に利用） --------\n",
    "MANUAL_PARAMS = dict(\n",
    "    backbone       = \"resnet18\",\n",
    "    layers         = (\"layer2\",),\n",
    "    n_features     = None,\n",
    ")\n",
    "\n",
    "# ------- calculation n_features -------\n",
    "def _canon(layer_name: str) -> str:\n",
    "    return layer_name.strip()\n",
    "\n",
    "def infer_n_features(backbone: str, layers: tuple) -> int:\n",
    "    model = timm.create_model(backbone, features_only=True, pretrained=True)\n",
    "    chan_map = {fi[\"module\"]: fi[\"num_chs\"] for fi in model.feature_info}\n",
    "    return sum(chan_map[_canon(l)] for l in layers)\n",
    "\n",
    "# -------- best_params.yaml 読み込み --------\n",
    "best_params_path = PARAM_DIR / \"best_params.yaml\"\n",
    "\n",
    "if best_params_path.exists():\n",
    "    cfg = yaml.safe_load(open(best_params_path))\n",
    "    print(\"▶ Using best_params.yaml:\", cfg)\n",
    "else:\n",
    "    cfg = MANUAL_PARAMS.copy()\n",
    "    print(\"▶ Using manual params:\", cfg)\n",
    "\n",
    "# -------- layers をタプル化 --------\n",
    "raw_layers = cfg[\"layers\"]\n",
    "if isinstance(raw_layers, (list, tuple)):\n",
    "    layers_tuple = tuple(raw_layers)\n",
    "else:\n",
    "    layers_tuple = eval(raw_layers)\n",
    "\n",
    "# -------- n_features を確定 --------\n",
    "if cfg.get(\"n_features\") is not None:\n",
    "    n_feat = cfg[\"n_features\"]\n",
    "elif best_params_path.exists():\n",
    "    _bp        = yaml.safe_load(open(best_params_path))\n",
    "    n_feat = int(_bp[\"n_features\"])\n",
    "else:\n",
    "    n_feat = infer_n_features(cfg[\"backbone\"], layers_tuple)\n",
    "\n",
    "# -------- Model & DataModule --------\n",
    "pre   = Padim.configure_pre_processor(image_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "model = Padim(\n",
    "    backbone               = cfg[\"backbone\"],\n",
    "    layers                 = layers_tuple,\n",
    "    n_features             = n_feat,\n",
    "    pre_processor          = pre,\n",
    ")\n",
    "dm = build_datamodule()\n",
    "\n",
    "# -------- Lightning学習 (GPU/CPUフォールバック) --------\n",
    "tb_logger = TensorBoardLogger(save_dir=RUN_DIR / \"logs\", name=\"final\")\n",
    "\n",
    "used_gpu  = None\n",
    "for use_gpu in (GPU_OK, False):\n",
    "    try:\n",
    "        engine = Engine(\n",
    "            default_root_dir=TEMP_DIR / \"train\",\n",
    "            accelerator        = \"gpu\" if use_gpu else \"cpu\",\n",
    "            max_epochs         = MAX_EPOCHS,\n",
    "            log_every_n_steps  = 1,\n",
    "            enable_progress_bar= False,\n",
    "            logger             = tb_logger,\n",
    "        )\n",
    "        engine.fit(model=model, datamodule=dm)\n",
    "        used_gpu = use_gpu\n",
    "        break\n",
    "    except RuntimeError as e:\n",
    "        if \"cudaGetDeviceCount\" in str(e):\n",
    "            print(\"⚠️ CUDA 初期化エラー → CPU でリトライ\")\n",
    "            continue\n",
    "        raise\n",
    "\n",
    "if not used_gpu:\n",
    "    print(\"⚠️ GPU 使用不可 → CPU で学習完了\")\n",
    "\n",
    "\n",
    "# -------- Stats(mu, inv_cov) 保存 --------\n",
    "# PadimModel.MultiVariateGaussian 取得\n",
    "gaussian = model.model.gaussian\n",
    "\n",
    "# inv_covariance がなければ停止\n",
    "if not hasattr(gaussian, \"inv_covariance\"):\n",
    "    raise RuntimeError(\"❌ PadimModel.gaussian に 'inv_covariance' が定義されていません\")\n",
    "\n",
    "# 平均ベクトルと逆共分散行列を取り出し\n",
    "mu      = gaussian.mean.cpu().numpy()             # shape: (n_features,)\n",
    "inv_cov = gaussian.inv_covariance.cpu().numpy()   # shape: (n_features, n_features)\n",
    "\n",
    "stats_path = RUN_DIR / \"pytorch\" / \"stats.npz\"\n",
    "np.savez_compressed(stats_path, mu=mu, inv_cov=inv_cov)\n",
    "print(\"✓ Stats(mu, inv_cov) :\", stats_path)\n",
    "\n",
    "# -------- Checkpoint 保存 --------\n",
    "ckpt_path = RUN_DIR / \"checkpoint\" / \"best.ckpt\"\n",
    "engine.trainer.save_checkpoint(ckpt_path)\n",
    "print(\"✓ Checkpoint :\", ckpt_path)\n",
    "\n",
    "# -------- 推論用モデル .pth 保存 -------- \n",
    "state_path = RUN_DIR / \"pytorch\" / \"model.pth\"\n",
    "torch.save(model.state_dict(), state_path)\n",
    "print(\"✓ Weights :\", state_path)\n",
    "\n",
    "# ======== 学習データのみ推論して image_min, image_max を取得 ========\n",
    "# 正規化のために推論処理へ連携する (別の方法 Z-Score(μ, σ)正規化もあり)\n",
    "print(f\"Computing image_min and image_max ...\")\n",
    "\n",
    "# Post-Processor の正規化だけオフにする\n",
    "model.post_processor.enable_normalization = False\n",
    "\n",
    "# DataModule を train のみでセットアップ\n",
    "dm.setup(\"fit\")                         # train フェーズ相当\n",
    "predict_loader = dm.train_dataloader()  # train データを予測\n",
    "\n",
    "# Engine.predict で生マップを取得\n",
    "scores = []\n",
    "logging.getLogger(\"anomalib.visualization.image.item_visualizer\").setLevel(logging.ERROR)\n",
    "for batch in engine.predict(model=model, datamodule=dm, return_predictions=True):\n",
    "    # batch は list of ImagePrediction\n",
    "    for item in batch:\n",
    "        # 生の anomaly_map を最大値で画像ごとの raw スコアに\n",
    "        raw = float(item.anomaly_map.max().cpu())\n",
    "        scores.append(raw)\n",
    "\n",
    "# 正規化機構を元に戻す\n",
    "model.post_processor.enable_normalization = True\n",
    "\n",
    "# image_min と image_max を計算\n",
    "image_min = float(np.min(scores))\n",
    "image_max = float(np.max(scores))\n",
    "print(f\"Computed image_min: {image_min}, image_max: {image_max}\")\n",
    "# ==================================================================\n",
    "\n",
    "# ------- .pth に含まれないメタ情報を出力(json) -------\n",
    "import json\n",
    "\n",
    "meta_path = RUN_DIR / \"pytorch\" / \"meta.json\"\n",
    "meta = {\n",
    "    \"backbone\"            : cfg[\"backbone\"],\n",
    "    \"layers\"              : list(layers_tuple),\n",
    "    \"n_features\"          : int(n_feat),\n",
    "    \"image_size\"          : IMAGE_SIZE,\n",
    "    \"weights_pth\"         : state_path.name,\n",
    "    \"stats\"               : \"stats.npz\",\n",
    "    \"image_threshold\"     : IMAGE_THRESHOLD,  # 手動設定\n",
    "    \"image_threshold_auto\": IMAGE_THRESHOLD,  # Test後に上書き用(任意)\n",
    "    \"image_min\"           : image_min,\n",
    "    \"image_max\"           : image_max,\n",
    "}\n",
    "\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✓ Meta JSON :\", meta_path)\n",
    "\n",
    "# ---------- TEMP_DIR のクリーンアップ ----------\n",
    "# Debugする場合は、コメントアウトしてください。\n",
    "import shutil, gc\n",
    "if TEMP_DIR.exists():\n",
    "    # Windows でハンドルが残ると削除に失敗することがあるので、念のため GC\n",
    "    gc.collect()\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebafd68-af71-4cf4-bc00-d564fb742559",
   "metadata": {},
   "source": [
    "## 4. 検出性能テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fffb21-99c0-4c1b-90ad-45a649fff250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from anomalib.models import Padim\n",
    "from anomalib.engine import Engine\n",
    "import torch, pandas as pd, shutil\n",
    "import logging \n",
    "\n",
    "# ---------- パス・デバイス ----------\n",
    "state_path  = Path(RUN_DIR / \"pytorch\" / \"model.pth\")\n",
    "result_root = RUN_DIR / \"test_result\"\n",
    "device      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "n_feat = cfg.get(\"n_features\")\n",
    "\n",
    "# ---------- モデル ----------\n",
    "model = Padim(\n",
    "    backbone      = cfg[\"backbone\"],\n",
    "    layers        = layers_tuple,\n",
    "    n_features    = n_feat,\n",
    "    pre_processor = pre,\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(state_path, map_location=device), strict=True)\n",
    "\n",
    "# ---------- DataModule ----------\n",
    "dm = build_datamodule(); dm.setup(\"test\")\n",
    "\n",
    "# ---------- テスト (Visualizer はデフォルトのまま有効) ----------\n",
    "logging.getLogger(\"anomalib.visualization.image.item_visualizer\").setLevel(logging.ERROR)\n",
    "engine = Engine(\n",
    "    accelerator=\"gpu\" if device == \"cuda\" else \"cpu\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=False,\n",
    "    logger=False,\n",
    ")\n",
    "engine.test(model=model, datamodule=dm)\n",
    "\n",
    "metrics = engine.trainer.callback_metrics.copy()\n",
    "auroc   = float(metrics.get(\"image_AUROC\", 0))\n",
    "f1      = float(metrics.get(\"image_F1Score\", 0))\n",
    "\n",
    "# testで自動決定される image_threshold \n",
    "threshold = float(getattr(model.post_processor, \"image_threshold\", IMAGE_THRESHOLD))\n",
    "\n",
    "# ---------- 可視化画像を result_root へ集約 ----------\n",
    "# Visualizer は  results/PaDiM/<CATEGORY>/<timestamp>/images/*\n",
    "default_root = Path(\"results\") / \"PaDiM\" / CATEGORY\n",
    "if default_root.exists():\n",
    "    # 直近 (mtime が最大) の test_run を取得\n",
    "    latest = max(default_root.iterdir(), key=lambda p: p.stat().st_mtime)\n",
    "    src_images = latest / \"images\"                    # normal / anomaly\n",
    "    dst_images = result_root / \"images\"\n",
    "    for lbl_dir in src_images.glob(\"*\"):              # normal & anomaly\n",
    "        (dst_images / lbl_dir.name).mkdir(parents=True, exist_ok=True)\n",
    "        for img in lbl_dir.glob(\"*\"):\n",
    "            shutil.copy2(img, dst_images / lbl_dir.name / img.name)\n",
    "\n",
    "# ---------- 1枚ずつチェック ----------\n",
    "def get_gt(item: \"ImagePrediction\") -> int:\n",
    "    \"\"\"0=normal, 1=anomaly を返す。属性が無ければパスで判定\"\"\"\n",
    "    for key in (\"label\", \"labels\", \"is_anomaly\", \"targets\"):\n",
    "        if hasattr(item, key):\n",
    "            return int(getattr(item, key))\n",
    "\n",
    "    # fallback\n",
    "    parent = Path(item.image_path).parent.name.lower()\n",
    "    return 1 if parent == \"anomaly\" else 0\n",
    "\n",
    "records = []\n",
    "for batch in engine.predict(model=model, datamodule=dm):\n",
    "    for item in batch:\n",
    "        file  = Path(item.image_path).name\n",
    "        score = float(item.pred_score)\n",
    "        pred  = \"anomaly\" if int(item.pred_label) == 1 else \"normal\"\n",
    "        gt    = \"anomaly\" if get_gt(item) else \"normal\"\n",
    "        print(f\"{file:40s} | score={score:7.4f} | pred={pred:7s} | label={gt}\")\n",
    "        records.append(dict(file=file, score=score, pred=pred, label=gt))\n",
    "\n",
    "# ---------- CSV 保存 ----------\n",
    "result_root.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = result_root / \"predictions.csv\"\n",
    "pd.DataFrame(records).to_csv(csv_path, index=False)\n",
    "print(f\"\\n✓ predictions.csv saved to {csv_path}\\n\")\n",
    "\n",
    "# ---------- meta.json 更新 (image_threshold_auto) ----------\n",
    "meta_path = Path(RUN_DIR) / \"pytorch\" / \"meta.json\"\n",
    "with open(meta_path, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "meta[\"image_threshold_auto\"] = float(threshold)\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✓ threshold_auto updated to {meta_path}\")\n",
    "\n",
    "# ---------- 指標 ----------\n",
    "print(\"\\n==========  EVALUATION  ==========\")\n",
    "print(f\"Images tested : {len(dm.test_data)}\")\n",
    "print(f\"AUROC         : {auroc:7.4f}\")\n",
    "print(f\"Best F1       : {f1:7.4f}\")\n",
    "print(\"===================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca3504-93f8-44b6-b852-c91487c94dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

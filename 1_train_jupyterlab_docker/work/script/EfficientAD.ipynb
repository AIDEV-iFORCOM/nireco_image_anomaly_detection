{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7017c035-1de7-4e6c-ac3b-0d71fd54ab5e",
   "metadata": {},
   "source": [
    "# EfficientAD Training\n",
    "\n",
    "- EfficientADモデルを単一カテゴリデータ(Normal)で学習します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc0d60a-0984-4cf7-bdc4-fd6be204e5fa",
   "metadata": {},
   "source": [
    "### （事前）CUDA Version 確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc559f6-5449-4743-9576-200dd651c668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch, platform\n",
    "print(\"torch version :\", torch.__version__)\n",
    "print(\"cuda in torch :\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a317c868-153a-48ce-a56f-a3a1430d7e3c",
   "metadata": {},
   "source": [
    "## 1. 初期設定\n",
    "\n",
    "- 処理する画像サイズ 「IMAGE_SIZE」 と、検出対象とする画像カテゴリ 「CATEGORY」を指定してください\n",
    "- 作成された「RUN_DIR」に、学習済モデルが作成されます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b25992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from collections import defaultdict\n",
    "import yaml\n",
    "import optuna\n",
    "import torch\n",
    "import timm, itertools, re\n",
    "\n",
    "from anomalib.data import Folder\n",
    "from anomalib.models import Padim\n",
    "from anomalib.engine import Engine\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "# -------- ユーザ設定項目 --------\n",
    "IMAGE_SIZE       = 256                         # 画像サイズ(★変更対象)\n",
    "IMAGE_THRESHOLD  = 0.5                         # 異常検出閾値(★変更対象)\n",
    "CATEGORY         = \"VisA_pipe_fryum\"           # 検出対象のカテゴリ(★変更対象)\n",
    "DATA_ROOT        = Path(\"/workspace/data\")     # データフォルダ(tarin/test)\n",
    "# -------------------------------\n",
    "\n",
    "JST = timezone(timedelta(hours=9))\n",
    "timestamp = datetime.now(JST).strftime('%Y%m%d_%H%M%S')       # 実行時のシステム日時\n",
    "OUTPUT_DIR  = Path(\"/workspace/models\") / \"EfficientAD\" / CATEGORY  # 学習済モデルの出力先\n",
    "RUN_DIR   = OUTPUT_DIR / timestamp\n",
    "(RUN_DIR / 'checkpoint').mkdir(parents=True, exist_ok=True)\n",
    "(RUN_DIR / 'pytorch').mkdir(parents=True, exist_ok=True)\n",
    "TEMP_DIR = RUN_DIR / \"temp\"\n",
    "PARAM_DIR = RUN_DIR / \"param\"\n",
    "LOG_DIR = RUN_DIR / \"logs\"\n",
    "\n",
    "print('RUN DIR:', RUN_DIR)\n",
    "\n",
    "# -------- DataModule定義 --------\n",
    "def build_datamodule() -> Folder:\n",
    "    return Folder(\n",
    "        name=CATEGORY,\n",
    "        root=DATA_ROOT,\n",
    "        normal_dir=f\"train/{CATEGORY}\",             # 学習用正常データ\n",
    "        abnormal_dir=f\"test/{CATEGORY}/anomaly\",    # テスト(評価)用異常データ (学習では使われない/Optuna探索では使われる)\n",
    "        normal_test_dir=f\"test/{CATEGORY}/normal\",  # テスト(評価)用正常データ\n",
    "        train_batch_size=1,                         # EfficientADでは、1固定 \n",
    "        eval_batch_size=32,\n",
    "        extensions=(\n",
    "            \".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\",\n",
    "            \".JPG\", \".JPEG\", \".PNG\", \".BMP\", \".TIF\", \".TIFF\", \".WEBP\",\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd2fc0",
   "metadata": {},
   "source": [
    "## 2. ハイパーパラメータ探索 (Optuna)\n",
    "\n",
    "- ここでは、ハイパーパラメータの自動探索を行います\n",
    "- PatchCore は、本来完全教師なし学習ですが、Optuna 探索は検証用に異常ラベルを必要とする教師ありプロセスと言えます\n",
    "- 探索を行わずに手動でハイパーパラメータを調整する場合は、ここをスキップして「3. 学習 」へ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78607bf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint \n",
    "from anomalib.models import EfficientAd\n",
    "from anomalib.engine import Engine\n",
    "import optuna, torch, yaml\n",
    "from PIL import ImageFile\n",
    "\n",
    "# -------- GPU利用可否 --------\n",
    "GPU_OK = torch.cuda.is_available()\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True      # 破損した画像も取込む(ロジックとして除去するのもあり)\n",
    "\n",
    "# -------- サーチスペース(★変更対象) --------\n",
    "N_TRIALS    = 3\n",
    "MODEL_SIZES = [\"small\", \"medium\"]           # largeは無し\n",
    "LR_RANGE    = (1e-4, 1e-3)\n",
    "\n",
    "# ---------- EfficientAdラッパー ----------\n",
    "class EfficientAdNoIter(EfficientAd):\n",
    "    def on_save_checkpoint(self, checkpoint):\n",
    "        # pickle 化不可能なオブジェクトを取り除く\n",
    "        # NotImplementedError: ('{} cannot be pickled', '_SingleProcessDataLoaderIter') 対策        \n",
    "        self.__dict__.pop(\"imagenet_iterator\", None)\n",
    "        return super().on_save_checkpoint(checkpoint)\n",
    "    def on_load_checkpoint(self, checkpoint):\n",
    "        pass\n",
    "        \n",
    "def objective(trial: optuna.Trial):\n",
    "    model_size = trial.suggest_categorical(\"model_size\", MODEL_SIZES)\n",
    "    lr         = trial.suggest_float(\"learning_rate\", *LR_RANGE, log=True)\n",
    "\n",
    "    dm  = build_datamodule()                       # build_datamodule内で train_batch_size=1 が必須\n",
    "    pre = EfficientAd.configure_pre_processor(image_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "    model = EfficientAdNoIter(                     # ラッパー利用\n",
    "        model_size=model_size, \n",
    "        lr=lr, \n",
    "        pre_processor=pre\n",
    "        #imagenet_dir=\"path/to/local/imagenette\"  # 事前学習画像のDLを外したい場合に指定\n",
    "    )\n",
    "  \n",
    "    # -------------- Engine -----------------\n",
    "    engine = Engine(\n",
    "        default_root_dir    = TEMP_DIR / \"optuna\",\n",
    "        accelerator         = \"gpu\" if GPU_OK else \"cpu\",\n",
    "        max_epochs          = 1,\n",
    "        logger              = False,\n",
    "        enable_progress_bar = False,\n",
    "    )\n",
    "\n",
    "    # ---------- GPU/CPUフォールバック ----------\n",
    "    used_gpu = None\n",
    "    for use_gpu in (GPU_OK, False):\n",
    "        try:\n",
    "            engine.fit(model=model, datamodule=dm)\n",
    "            used_gpu = use_gpu\n",
    "            break\n",
    "        except RuntimeError as e:\n",
    "            if \"cudaGetDeviceCount\" in str(e):\n",
    "                print(\"⚠️ CUDA 初期化失敗 → CPU で再試行\")\n",
    "                engine._cache.args[\"accelerator\"] = \"cpu\"\n",
    "                continue\n",
    "            raise\n",
    "\n",
    "    trial.set_user_attr(\"used_gpu\", used_gpu)\n",
    "    if not used_gpu:\n",
    "        print(f\"Trial {trial.number}: CPU で実行 (GPU fallback)\")\n",
    "\n",
    "    # 取得できない場合は 0.0\n",
    "    return engine.trainer.callback_metrics.get(\"image_AUROC\",\n",
    "                                               torch.tensor(0.)).item()\n",
    "# -------- Optuna 実行 --------\n",
    "TEMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=N_TRIALS)\n",
    "\n",
    "print(\"BEST :\", study.best_params)\n",
    "print(\"AUROC:\", study.best_value)\n",
    "\n",
    "# -------- 結果保存 --------\n",
    "PARAM_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "search_space = dict(\n",
    "    model_size    = MODEL_SIZES,\n",
    "    learning_rate = {\"low\": LR_RANGE[0], \"high\": LR_RANGE[1]},\n",
    ")\n",
    "yaml.safe_dump(search_space, open(PARAM_DIR / \"search_space.yaml\", \"w\"))\n",
    "\n",
    "best = study.best_params.copy()\n",
    "yaml.safe_dump(best, open(PARAM_DIR / \"best_params.yaml\", \"w\"))\n",
    "study.trials_dataframe().to_csv(PARAM_DIR / \"trials.csv\", index=False)\n",
    "\n",
    "# ---------- TEMP_DIR のクリーンアップ ----------\n",
    "# Debugする場合は、コメントアウトしてください\n",
    "import shutil, gc\n",
    "if TEMP_DIR.exists():\n",
    "    # Windows でハンドルが残ると削除に失敗することがあるので、念のため GC\n",
    "    gc.collect()\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73415335",
   "metadata": {},
   "source": [
    "## 3. 学習\n",
    "\n",
    "- 「2. ハイパーパラメータ探索(Optuna)」 を行った場合は、保存されたベストパラメータで学習します\n",
    "- 自動探索を行わない行場合は、ハイパーパラメータ 「MANUAL_PARAMS」を手動で調整してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1891f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor\n",
    "import numpy as np, torch, yaml, types\n",
    "from anomalib.models import EfficientAd\n",
    "from anomalib.engine import Engine\n",
    "from anomalib.deploy import ExportType\n",
    "\n",
    "# -------- GPU 利用可否 --------\n",
    "GPU_OK = torch.cuda.is_available()\n",
    "\n",
    "# -------- 学習設定 --------\n",
    "MAX_EPOCHS = 1  # ※固定\n",
    "\n",
    "# -------- ★手動パラメータ（best_params.yaml が無い時に使用） --------\n",
    "MANUAL_PARAMS = dict(\n",
    "    model_size    = \"medium\",      # \"small\" or \"medium\"\n",
    "    learning_rate = 1e-4,\n",
    ")\n",
    "\n",
    "# ---------- EfficientAdラッパー ----------\n",
    "class EfficientAdNoIter(EfficientAd):\n",
    "    def on_save_checkpoint(self, checkpoint):\n",
    "        # pickle 化不可能なオブジェクトを取り除く\n",
    "        # NotImplementedError: ('{} cannot be pickled', '_SingleProcessDataLoaderIter') 対策        \n",
    "        self.__dict__.pop(\"imagenet_iterator\", None)\n",
    "        return super().on_save_checkpoint(checkpoint)\n",
    "    def on_load_checkpoint(self, checkpoint):\n",
    "        pass\n",
    "\n",
    "# -------- best_params.yaml 読み込み --------\n",
    "best_params_path = PARAM_DIR / \"best_params.yaml\"\n",
    "\n",
    "if best_params_path.exists():\n",
    "    cfg = yaml.safe_load(open(best_params_path))\n",
    "    print(\"▶ Using best_params.yaml:\", cfg)\n",
    "else:\n",
    "    cfg = MANUAL_PARAMS.copy()\n",
    "    print(\"▶ Using manual params:\", cfg)\n",
    "\n",
    "# -------- Model & DataModule --------\n",
    "dm  = build_datamodule()            # build_datamodule内で train_batch_size=1 が必須\n",
    "\n",
    "# -------- モデル定義 --------\n",
    "pre   = EfficientAd.configure_pre_processor(image_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "model = EfficientAdNoIter(\n",
    "    model_size   = cfg[\"model_size\"],\n",
    "    lr           = cfg[\"learning_rate\"],\n",
    "    pre_processor= pre,\n",
    ")\n",
    "# -------- Lightning学習 (GPU/CPUフォールバック) --------\n",
    "tb_logger = TensorBoardLogger(save_dir=RUN_DIR / \"logs\", name=\"final\")\n",
    "used_gpu  = None\n",
    "for use_gpu in (GPU_OK, False):\n",
    "    try:\n",
    "        engine = Engine(\n",
    "            default_root_dir   = TEMP_DIR / \"train\",\n",
    "            accelerator        = \"gpu\" if use_gpu else \"cpu\",\n",
    "            max_epochs         = MAX_EPOCHS,\n",
    "            log_every_n_steps  = 1,\n",
    "            enable_progress_bar= False,\n",
    "            logger             = tb_logger,\n",
    "        )\n",
    "        engine.fit(model=model, datamodule=dm)\n",
    "        used_gpu = use_gpu\n",
    "        break\n",
    "    except RuntimeError as e:\n",
    "        if \"cudaGetDeviceCount\" in str(e):\n",
    "            warnings.warn(\"⚠️ CUDA 初期化エラー → CPU で再試行\")\n",
    "            continue\n",
    "        raise\n",
    "if not used_gpu:\n",
    "    print(\"⚠️ GPU 使用不可 → CPU で学習完了\")\n",
    "\n",
    "# -------- Checkpoint 保存 --------\n",
    "ckpt_path = RUN_DIR / \"checkpoint\" / \"best.ckpt\"\n",
    "engine.trainer.save_checkpoint(ckpt_path)\n",
    "print(\"✓ Checkpoint :\", ckpt_path)\n",
    "\n",
    "# -------- 推論用モデル .pth 保存 -------- \n",
    "state_path = RUN_DIR / \"pytorch\" / \"model.pth\"\n",
    "torch.save(model.state_dict(), state_path)\n",
    "print(\"✓ Weights :\", state_path)\n",
    "\n",
    "# ======== 学習データのみ推論して image_min, image_max を取得 ========\n",
    "# 正規化のために推論処理へ連携する (別の方法 Z-Score(μ, σ)正規化もあり)\n",
    "print(f\"Computing image_min and image_max ...\")\n",
    "\n",
    "# Post-Processor の正規化だけオフにする\n",
    "model.post_processor.enable_normalization = False\n",
    "\n",
    "# DataModule を train のみでセットアップ\n",
    "dm.setup(\"fit\")                         # train フェーズ相当\n",
    "predict_loader = dm.train_dataloader()  # train データを予測\n",
    "\n",
    "# Engine.predict で生マップを取得\n",
    "scores = []\n",
    "logging.getLogger(\"anomalib.visualization.image.item_visualizer\").setLevel(logging.ERROR)\n",
    "for batch in engine.predict(model=model, datamodule=dm, return_predictions=True):\n",
    "    # batch は list of ImagePrediction\n",
    "    for item in batch:\n",
    "        # 生の anomaly_map を最大値で画像ごとの raw スコアに\n",
    "        raw = float(item.anomaly_map.max().cpu())\n",
    "        scores.append(raw)\n",
    "\n",
    "# 正規化機構を元に戻す\n",
    "model.post_processor.enable_normalization = True\n",
    "\n",
    "# image_min と image_max を計算\n",
    "image_min = float(np.min(scores))\n",
    "image_max = float(np.max(scores))\n",
    "print(f\"Computed image_min: {image_min}, image_max: {image_max}\")\n",
    "# ==================================================================\n",
    "\n",
    "# ------- .pth に含まれないメタ情報を出力(json) -------\n",
    "import json\n",
    "\n",
    "meta_path = RUN_DIR / \"pytorch\" / \"meta.json\"\n",
    "meta = {\n",
    "    \"model_size\"          : cfg[\"model_size\"],\n",
    "    \"learning_rate\"       : cfg[\"learning_rate\"],\n",
    "    \"image_size\"          : IMAGE_SIZE,\n",
    "    \"weights_pth\"         : state_path.name,\n",
    "    \"image_threshold\"     : IMAGE_THRESHOLD,  # 手動設定\n",
    "    \"image_threshold_auto\": IMAGE_THRESHOLD,  # Test後に上書き用(任意)\n",
    "    \"image_min\"       : image_min,\n",
    "    \"image_max\"       : image_max,    \n",
    "}\n",
    "\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✓ Meta JSON :\", meta_path)\n",
    "\n",
    "# ---------- TEMP_DIR のクリーンアップ ----------\n",
    "# Debugする場合は、コメントアウトしてください。\n",
    "import shutil, gc\n",
    "if TEMP_DIR.exists():\n",
    "    # Windows でハンドルが残ると削除に失敗することがあるので、念のため GC\n",
    "    gc.collect()\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebafd68-af71-4cf4-bc00-d564fb742559",
   "metadata": {},
   "source": [
    "## 4. 検出性能テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fffb21-99c0-4c1b-90ad-45a649fff250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from anomalib.models import EfficientAd\n",
    "from anomalib.engine import Engine\n",
    "import torch, pandas as pd, shutil\n",
    "import logging \n",
    "\n",
    "# ---------- パス・デバイス ----------\n",
    "state_path  = Path(RUN_DIR / \"pytorch\" / \"model.pth\")\n",
    "result_root = RUN_DIR / \"test_result\"\n",
    "device      = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------- モデル ----------\n",
    "pre   = EfficientAd.configure_pre_processor(image_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
    "model = EfficientAd(\n",
    "    model_size   = cfg[\"model_size\"],\n",
    "    lr           = cfg[\"learning_rate\"],\n",
    "    pre_processor= pre,\n",
    ").to(device)\n",
    "model.load_state_dict(torch.load(state_path, map_location=device), strict=True)\n",
    "\n",
    "# ---------- DataModule ----------\n",
    "dm = build_datamodule(); dm.setup(\"test\")\n",
    "\n",
    "# suppress visualizer logs\n",
    "logging.getLogger(\"anomalib.visualization.image.item_visualizer\").setLevel(logging.ERROR)\n",
    "\n",
    "# ---------- テスト (Visualizer はデフォルトのまま有効) ----------\n",
    "engine = Engine(\n",
    "    accelerator=\"gpu\" if device == \"cuda\" else \"cpu\",\n",
    "    devices=1,\n",
    "    enable_progress_bar=False,\n",
    "    logger=False,\n",
    ")\n",
    "engine.test(model=model, datamodule=dm)\n",
    "\n",
    "metrics = engine.trainer.callback_metrics.copy()          # predict 前に退避\n",
    "auroc   = float(metrics.get(\"image_AUROC\", 0))\n",
    "f1      = float(metrics.get(\"image_F1Score\", 0))\n",
    "\n",
    "# testで自動決定される image_threshold \n",
    "threshold = float(getattr(model.post_processor, \"image_threshold\", IMAGE_THRESHOLD))\n",
    "\n",
    "# ---------- 可視化画像を result_root へ集約 ----------\n",
    "# Visualizer は  results/EfficientAD/<CATEGORY>/<timestamp>/images/*\n",
    "default_root = Path(\"results\") / \"EfficientAD\" / CATEGORY\n",
    "if default_root.exists():\n",
    "    # 直近 (mtime が最大) の test_run を取得\n",
    "    latest = max(default_root.iterdir(), key=lambda p: p.stat().st_mtime)\n",
    "    src_images = latest / \"images\"                    # normal / anomaly\n",
    "    dst_images = result_root / \"images\"\n",
    "    for lbl_dir in src_images.glob(\"*\"):              # normal & anomaly\n",
    "        (dst_images / lbl_dir.name).mkdir(parents=True, exist_ok=True)\n",
    "        for img in lbl_dir.glob(\"*\"):\n",
    "            shutil.copy2(img, dst_images / lbl_dir.name / img.name)\n",
    "\n",
    "# ---------- 1枚ずつ結果を表示  ----------\n",
    "def get_gt(item: \"ImagePrediction\") -> int:\n",
    "    \"\"\"0=normal, 1=anomaly を返す。属性が無ければパスで判定\"\"\"\n",
    "    for key in (\"label\", \"labels\", \"is_anomaly\", \"targets\"):\n",
    "        if hasattr(item, key):\n",
    "            return int(getattr(item, key))\n",
    "\n",
    "    # fallback\n",
    "    parent = Path(item.image_path).parent.name.lower()\n",
    "    return 1 if parent == \"anomaly\" else 0\n",
    "\n",
    "records = []\n",
    "for batch in engine.predict(model=model, datamodule=dm, return_predictions=True):\n",
    "    for item in batch:\n",
    "        file  = Path(item.image_path).name\n",
    "        score = float(item.pred_score)\n",
    "        pred  = \"anomaly\" if int(item.pred_label) == 1 else \"normal\"\n",
    "        gt    = \"anomaly\" if get_gt(item) else \"normal\"\n",
    "        print(f\"{file:40s} | score={score:7.4f} | pred={pred:7s} | label={gt}\")\n",
    "        records.append(dict(file=file, score=score, pred=pred, label=gt))\n",
    "\n",
    "# ---------- CSV 保存 ----------\n",
    "result_root.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = result_root / \"predictions.csv\"\n",
    "pd.DataFrame(records).to_csv(csv_path, index=False)\n",
    "print(f\"\\n✓ predictions.csv saved to {csv_path}\\n\")\n",
    "\n",
    "# ---------- meta.json 更新 (image_threshold_auto) ----------\n",
    "meta_path = Path(RUN_DIR) / \"pytorch\" / \"meta.json\"\n",
    "with open(meta_path, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "meta[\"image_threshold_auto\"] = float(threshold)\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✓ threshold_auto updated to {meta_path}\")\n",
    "\n",
    "# ---------- 指標 ----------\n",
    "print(\"\\n==========  EVALUATION  ==========\")\n",
    "print(f\"Images tested : {len(dm.test_data)}\")\n",
    "print(f\"AUROC         : {auroc:7.4f}\")\n",
    "print(f\"Best F1       : {f1:7.4f}\")\n",
    "print(\"===================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e764de8-1b07-43d6-a284-0388a0b9f765",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

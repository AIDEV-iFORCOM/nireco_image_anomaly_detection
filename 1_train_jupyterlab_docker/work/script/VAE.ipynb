{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7017c035-1de7-4e6c-ac3b-0d71fd54ab5e",
   "metadata": {},
   "source": [
    "# VAE(Variational Autoencoder) Training\n",
    "\n",
    "- VAEモデルを単一カテゴリデータ(Normal)で学習します\n",
    "- 推論には 「latent_dim」 が必要になります"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc0d60a-0984-4cf7-bdc4-fd6be204e5fa",
   "metadata": {},
   "source": [
    "### （事前）CUDA Version 確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc559f6-5449-4743-9576-200dd651c668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch, platform\n",
    "print(\"torch version :\", torch.__version__)\n",
    "print(\"cuda in torch :\", torch.version.cuda)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a317c868-153a-48ce-a56f-a3a1430d7e3c",
   "metadata": {},
   "source": [
    "## 1. 初期設定\n",
    "\n",
    "- 処理する画像サイズ 「IMAGE_SIZE」 と、検出対象とする画像カテゴリ 「CATEGORY」を指定してください\n",
    "- 作成された「RUN_DIR」に、学習済モデルが作成されます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b25992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import yaml, optuna, torch, shutil\n",
    "from collections import defaultdict\n",
    "from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "from torch import nn\n",
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from anomalib.data import Folder\n",
    "from typing import Sequence\n",
    "from collections.abc import Sequence\n",
    "\n",
    "# -------- ユーザ設定項目 --------\n",
    "IMAGE_SIZE  = 256                              # 画像サイズ(★変更対象)\n",
    "IMAGE_THRESHOLD  = 0.5                         # 異常検出閾値(★変更対象)\n",
    "CATEGORY    = \"VisA_pipe_fryum\"                # 検出対象のカテゴリ(★変更対象)\n",
    "DATA_ROOT   = Path(\"/workspace/data\")          # データフォルダ(train/test)\n",
    "# -------------------------------\n",
    "\n",
    "JST = timezone(timedelta(hours=9))\n",
    "timestamp  = datetime.now(JST).strftime('%Y%m%d_%H%M%S')\n",
    "OUTPUT_DIR = Path(\"/workspace/models\") / \"VAE\" / CATEGORY\n",
    "RUN_DIR    = OUTPUT_DIR / timestamp\n",
    "(RUN_DIR / 'checkpoint').mkdir(parents=True, exist_ok=True)\n",
    "(RUN_DIR / 'pytorch').mkdir(parents=True, exist_ok=True)\n",
    "TEMP_DIR = RUN_DIR / \"temp\"\n",
    "PARAM_DIR = RUN_DIR / \"param\"\n",
    "LOG_DIR = RUN_DIR / \"logs\"\n",
    "print(\"RUN DIR:\", RUN_DIR)\n",
    "\n",
    "# -------- DataModule定義 --------\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as Ftv\n",
    "\n",
    "def build_datamodule(batch_size: int = 32) -> Folder:\n",
    "    \"\"\"\n",
    "    API 互換のため image_size / transform を渡さない\n",
    "    Folder 内部で Albumentations → ToTensorV2 が適用され Torch.Tensor が返る\n",
    "    \"\"\"\n",
    "    return Folder(\n",
    "        name=CATEGORY,\n",
    "        root=DATA_ROOT,\n",
    "        normal_dir=f\"train/{CATEGORY}\",\n",
    "        abnormal_dir=f\"test/{CATEGORY}/anomaly\",   # 評価用に異常も渡す（学習には使われない）\n",
    "        normal_test_dir=f\"test/{CATEGORY}/normal\",\n",
    "        train_batch_size=batch_size,\n",
    "        eval_batch_size=batch_size,\n",
    "        #train_num_workers=0,         # 共有メモリを使わない場合\n",
    "        #eval_num_workers=0,          # 共有メモリを使わない場合\n",
    "        extensions=(\n",
    "            \".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\",\n",
    "            \".JPG\", \".JPEG\", \".PNG\", \".BMP\", \".TIF\", \".TIFF\", \".WEBP\",\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "# -------- VAEモデル定義 --------\n",
    "class ConvVAE(LightningModule):\n",
    "    \"\"\"\n",
    "    * 入力: 3×IMAGE_SIZE×IMAGE_SIZE (0–1 float)\n",
    "    * 潜在: latent_dim (Optuna で探索)\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim: int = 128, lr: float = 1e-3, img_size: int = 256):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        chs = [3, 32, 64, 128]\n",
    "        enc = []\n",
    "        for cin, cout in zip(chs, chs[1:]):\n",
    "            enc += [nn.Conv2d(cin, cout, 4, 2, 1), nn.ReLU(inplace=True)]\n",
    "        self.encoder = nn.Sequential(*enc)\n",
    "\n",
    "        # --- 入出力次元を動的に算出 ------------------------------------\n",
    "        with torch.no_grad():\n",
    "            dummy   = torch.zeros(1, 3, img_size, img_size)\n",
    "            enc_out = self.encoder(dummy)\n",
    "        c, h, w     = enc_out.shape[1:]\n",
    "        flat_dim    = c * h * w\n",
    "\n",
    "        self._enc_shape = (c, h, w)           # decode 時に使用\n",
    "\n",
    "        self.mu     = nn.Linear(flat_dim, latent_dim)\n",
    "        self.logvar = nn.Linear(flat_dim, latent_dim)\n",
    "\n",
    "        self.fc_dec = nn.Linear(latent_dim, flat_dim)\n",
    "        dec = [\n",
    "            nn.ConvTranspose2d(c, 64, 4, 2, 1), nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 32, 4, 2, 1), nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(32,  3, 4, 2, 1), nn.Sigmoid()\n",
    "        ]\n",
    "        self.decoder = nn.Sequential(*dec)\n",
    "        self._img_size = img_size\n",
    "\n",
    "    # ----- forward 系 -----\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x).flatten(1)\n",
    "        return self.mu(h), self.logvar(h)\n",
    "\n",
    "    @staticmethod\n",
    "    def reparameterize(mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        return mu + torch.randn_like(std)*std\n",
    "\n",
    "    def decode(self, z):\n",
    "        c, h, w = self._enc_shape\n",
    "        h = self.fc_dec(z).view(z.size(0), c, h, w)\n",
    "        return self.decoder(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "    # ---------- 画像Tensor整形 ----------\n",
    "    def _prepare_x(self, batch):\n",
    "        # (以下コメントもコードも元のまま: 処理は同じ)\n",
    "        if isinstance(batch, (list, tuple)):\n",
    "            img = batch[0]\n",
    "        elif isinstance(batch, dict):\n",
    "            img = batch.get(\"image\", next(iter(batch.values())))\n",
    "        else:\n",
    "            img = batch\n",
    "\n",
    "        cls_name = img.__class__.__name__\n",
    "        if cls_name.endswith(\"ImageBatch\"):\n",
    "            img = list(img)\n",
    "        elif cls_name.endswith(\"ImageItem\"):\n",
    "            if hasattr(img, \"tensor\"):\n",
    "                img = img.tensor\n",
    "            elif hasattr(img, \"data\"):\n",
    "                img = img.data\n",
    "            elif hasattr(img, \"image\"):\n",
    "                img = img.image\n",
    "\n",
    "        if isinstance(img, Sequence) and not isinstance(img, (torch.Tensor, np.ndarray)):\n",
    "            tensors = [self._prepare_x([sub]) for sub in img]\n",
    "            return torch.cat(tensors, dim=0)\n",
    "\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            x = img\n",
    "        elif isinstance(img, np.ndarray):\n",
    "            if img.dtype == object:\n",
    "                x = torch.stack([to_tensor(el) for el in img], dim=0)\n",
    "            else:\n",
    "                x = torch.from_numpy(img)\n",
    "        else:\n",
    "            x = to_tensor(img)\n",
    "\n",
    "        if x.dtype == torch.uint8:\n",
    "            x = x.float().div_(255)\n",
    "        elif x.is_floating_point() and x.max() > 1.5:\n",
    "            x = x.div_(255)\n",
    "\n",
    "        if x.ndim == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "        elif x.ndim == 4 and x.shape[-1] in (1, 3):\n",
    "            x = x.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        if x.shape[-2:] != (self._img_size, self._img_size):\n",
    "            x = F.interpolate(x, size=(self._img_size, self._img_size),\n",
    "                              mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        return x.contiguous()\n",
    "        \n",
    "    def _vae_loss(self, x_hat, x, mu, logvar):\n",
    "        recon = F.mse_loss(x_hat, x, reduction=\"mean\")\n",
    "        kl    = -0.5*torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return recon + kl*0.0005\n",
    "\n",
    "    def _shared_step(self, batch, stage):\n",
    "        x = self._prepare_x(batch)\n",
    "        x_hat, mu, logvar = self(x)\n",
    "        loss = self._vae_loss(x_hat, x, mu, logvar)\n",
    "        self.log(f\"{stage}_loss\", loss, prog_bar=False)\n",
    "        return loss, x, x_hat\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        loss, _, _ = self._shared_step(batch, \"train\")\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        # --- 画像を取り出す -------------------------------------------\n",
    "        if isinstance(batch, dict):\n",
    "            img_raw   = batch[\"image\"]                 # 画像 Tensor\n",
    "            path_list = batch.get(\"image_path\", [\"\"])  # list[str|Path] or str\n",
    "            if isinstance(path_list, (str, Path)):\n",
    "                path_list = [path_list]\n",
    "\n",
    "            # 正常=0 / 異常=1 のラベルを作成\n",
    "            y = torch.tensor(\n",
    "                [1.0 if (\"anomaly\" in str(p) or \"defect\" in str(p)) else 0.0\n",
    "                 for p in path_list],\n",
    "                device=self.device,\n",
    "                dtype=torch.float32,\n",
    "            )\n",
    "\n",
    "        elif isinstance(batch, (list, tuple)) and len(batch) == 2:\n",
    "            img_raw, y = batch                         # (img, label)\n",
    "            y = y.float().to(self.device)\n",
    "\n",
    "        else:                                          # ラベル情報なし → 正常扱い\n",
    "            img_raw = batch\n",
    "            # len(img_raw) でバッチ長を取得（ImageBatch に対応）\n",
    "            y = torch.zeros(len(img_raw), device=self.device)\n",
    "\n",
    "        # (_shared_step は画像だけ渡せば OK)\n",
    "        loss, x, x_hat = self._shared_step(img_raw, \"val\")\n",
    "\n",
    "        errs = F.mse_loss(x_hat, x, reduction=\"none\").mean([1, 2, 3])\n",
    "\n",
    "        # --- バッファに蓄積 -------------------------------------------\n",
    "        if not hasattr(self, \"_val_buf\"):\n",
    "            self._val_buf = defaultdict(list)\n",
    "        self._val_buf[\"errs\"].append(errs.detach())\n",
    "        self._val_buf[\"labels\"].append(y.detach())\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if not hasattr(self, \"_val_buf\") or len(self._val_buf[\"errs\"]) == 0:\n",
    "            return\n",
    "\n",
    "        errs   = torch.cat(self._val_buf[\"errs\"])\n",
    "        labels = torch.cat(self._val_buf[\"labels\"]).view(-1)\n",
    "\n",
    "        # --- AUROC 計算（正常=0, 異常=1） ----------------------------\n",
    "        if labels.unique().numel() < 2:\n",
    "            auroc = torch.tensor(0.5, device=self.device)\n",
    "        else:\n",
    "            auroc = torch.tensor(\n",
    "                roc_auc_score(labels.cpu(), errs.cpu()),\n",
    "                device=self.device\n",
    "            )\n",
    "\n",
    "        self.log(\"val_AUROC\", auroc, prog_bar=True)\n",
    "\n",
    "        # --- デバッグ用ログ -----------------------------------------\n",
    "        n_norm = int((labels == 0).sum())\n",
    "        n_anom = int((labels == 1).sum())\n",
    "        self.print(                                # ← Lightning の rank_zero_only print\n",
    "            f\"[VAL] epoch={self.current_epoch}  normals={n_norm}  \"\n",
    "            f\"anomalies={n_anom}  AUROC={auroc.item():.4f}\"\n",
    "        )\n",
    "\n",
    "        self._val_buf.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)\n",
    "\n",
    "# ------- Tensor化 (共通関数) -------\n",
    "def _to_tensor(img):\n",
    "    \"\"\"PIL / np.ndarray / torch.Tensor いずれも Tensor 化 (0-1)\"\"\"\n",
    "    import numpy as np\n",
    "    from torchvision.transforms.functional import to_tensor\n",
    "\n",
    "    if hasattr(img, \"tensor\"):\n",
    "        img = img.tensor\n",
    "    elif hasattr(img, \"image\"):\n",
    "        img = img.image\n",
    "        \n",
    "    if isinstance(img, torch.Tensor):\n",
    "        return img.float() / (255 if img.dtype == torch.uint8 else 1)\n",
    "\n",
    "    if isinstance(img, np.ndarray):\n",
    "        return torch.from_numpy(img).float() / (255 if img.dtype != np.float32 else 1)\n",
    "\n",
    "    return to_tensor(img)           # PIL など\n",
    "\n",
    "# ------- 画像&ラベル取出 (共通関数) -------\n",
    "def extract_xy(\n",
    "    batch,\n",
    "    device: torch.device | str = \"cpu\",\n",
    "    *,\n",
    "    return_counts: bool = False,\n",
    "    return_paths:  bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    ImageBatch / list[ImageItem] / dict のいずれが来ても以下を返す共通関数\n",
    "        x : Tensor[B,3,H,W]  (float32 0–1, device 移動済み)\n",
    "        y : Tensor[B]        (0:normal, 1:anomaly, float32, device 移動済み)\n",
    "        [counts]             n_norm, n_anom   ※ return_counts=True の時\n",
    "        [paths]              list[str|Path]   ※ return_paths =True の時\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : dict | ImageBatch | Sequence\n",
    "        DataLoader から受け取るバッチ\n",
    "    device : torch.device | str\n",
    "        `.to(device)` 先\n",
    "    return_counts : bool, default False\n",
    "        True のとき (n_norm, n_anom) を返す\n",
    "    return_paths : bool, default False\n",
    "        True のとき 画像パスの list を返す\n",
    "    \"\"\"\n",
    "    # ------ anomalib >=0.8 形式の dict -------\n",
    "    if isinstance(batch, dict):\n",
    "        x = batch[\"image\"]\n",
    "        y = batch[\"label\"].float()\n",
    "        paths = batch.get(\"image_path\", [\"\"] * len(y))\n",
    "\n",
    "    # ------ ImageBatch (list-like) または list[ImageItem] -------\n",
    "    else:\n",
    "        items: Sequence = list(batch)\n",
    "        x = torch.stack([_to_tensor(getattr(it, \"tensor\", it)) for it in items])\n",
    "        # ― ラベル取得 ―\n",
    "        if hasattr(items[0], \"label\"):\n",
    "            y = torch.tensor([float(it.label) for it in items])\n",
    "        else:\n",
    "            paths = [str(getattr(it, \"image_path\", \"\")) for it in items]\n",
    "            y = torch.tensor([1.0 if (\"anomaly\" in p or \"defect\" in p) else 0.0 for p in paths])\n",
    "\n",
    "    # ------ 後処理／戻り値組み立て -------\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    out  = [x, y]\n",
    "\n",
    "    if return_counts:\n",
    "        n_norm = int((y == 0).sum())\n",
    "        n_anom = int((y == 1).sum())\n",
    "        out.extend([n_norm, n_anom])\n",
    "\n",
    "    if return_paths:\n",
    "        out.append(paths)\n",
    "\n",
    "    return tuple(out) if len(out) > 1 else out[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd2fc0",
   "metadata": {},
   "source": [
    "## 2. ハイパーパラメータ探索 (Optuna)\n",
    "\n",
    "- ここでは、ハイパーパラメータの自動探索を行います\n",
    "- 探索方法は、以下より選択\n",
    "  - 正常画像のみ利用した MSE の最小化(完全教師無し)\n",
    "  - 正常画像と異常画像を両方利用する AUROC の最大化(教師ありプロセス)\n",
    "- 探索を行わずに手動でハイパーパラメータを調整する場合は、ここをスキップして「3. 学習」へ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78607bf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "import optuna, torch, yaml, types\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------- GPU利用可否 --------\n",
    "GPU_OK = torch.cuda.is_available()\n",
    "\n",
    "# -------- 探索に異常データ利用切替 --------\n",
    "'''\n",
    " True:  異常も使用   / 最適化指標: test AUROC / Validationなし\n",
    " False: 正常のみ使用 / 最適化指標: val MSE    / Validationあり\n",
    "'''\n",
    "EVAL_WITH_TEST_ANOM = True\n",
    "\n",
    "# -------- サーチスペース(★変更対象) --------\n",
    "N_TRIALS    = 1                           # 試行回数\n",
    "LATENT_OPTS = [64, 128, 256]              # 潜在次元候補\n",
    "LR_RANGE    = (1e-4, 5e-3)                # 学習率範囲\n",
    "\n",
    "def _to_tensor(item):\n",
    "    for attr in (\"tensor\", \"data\", \"image\"):\n",
    "        t = getattr(item, attr, None)\n",
    "        if t is not None:\n",
    "            return t\n",
    "    raise AttributeError(\"ImageItem に tensor/data/image 属性がありません\")\n",
    "\n",
    "def _evaluate_on_test(model, dm):\n",
    "    model.eval()\n",
    "    errs, labels = [], []\n",
    "    n_norm_tot = n_anom_tot = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b in dm.test_dataloader():\n",
    "            x, y, n_norm, n_anom = extract_xy(b, model.device, return_counts=True)\n",
    "            x = model._prepare_x(x) \n",
    "            xh, _, _ = model(x)\n",
    "            errs .append(F.mse_loss(xh, x, reduction=\"none\").mean([1,2,3]).cpu())\n",
    "            labels.append(y.cpu())\n",
    "            n_norm_tot += n_norm\n",
    "            n_anom_tot += n_anom\n",
    "\n",
    "    errs   = torch.cat(errs).numpy()\n",
    "    labels = torch.cat(labels).numpy()\n",
    "    auroc  = roc_auc_score(labels, errs)\n",
    "\n",
    "    print(f\"[TEST] normals={n_norm_tot}  anomalies={n_anom_tot}  AUROC={auroc:.4f}\")\n",
    "    return auroc\n",
    "    \n",
    "def _evaluate_on_val(model, dm):\n",
    "    model.eval()\n",
    "    mse = []; n_img = 0\n",
    "    with torch.no_grad():\n",
    "        for b in dm.val_dataloader():\n",
    "            x, _, n_norm, _ = extract_xy(b, model.device, return_counts=True)   # val は正常のみ\n",
    "            x = model._prepare_x(x) \n",
    "            xh, _, _ = model(x)\n",
    "            mse.append(F.mse_loss(xh, x, reduction=\"none\").mean([1,2,3]).cpu())\n",
    "            n_img += n_norm\n",
    "    mse_mean = torch.cat(mse).mean().item()\n",
    "    print(f\"[VAL] normals={n_img}  mean-MSE={mse_mean:.6f}\")\n",
    "    return mse_mean\n",
    "\n",
    "# -------- Objective --------\n",
    "def objective(trial: optuna.Trial):\n",
    "\n",
    "    latent_dim = trial.suggest_categorical(\"latent_dim\", LATENT_OPTS)\n",
    "    lr         = trial.suggest_float(\"lr\", *LR_RANGE, log=True)\n",
    "\n",
    "    # ---------- DataModule ----------\n",
    "    dm = build_datamodule(batch_size=32)\n",
    "    dm.setup()\n",
    "\n",
    "    # ---------- モデル ----------\n",
    "    model = ConvVAE(latent_dim=latent_dim, lr=lr)\n",
    "\n",
    "    # ---------- Trainer ----------\n",
    "    if EVAL_WITH_TEST_ANOM:      # test で評価 → validation ループ不要\n",
    "        trainer = Trainer(\n",
    "            logger=False,\n",
    "            default_root_dir=TEMP_DIR / \"optuna\",\n",
    "            accelerator=\"gpu\" if GPU_OK else \"cpu\",\n",
    "            max_epochs=5,\n",
    "            limit_val_batches=0,          # validation を回さない\n",
    "            enable_progress_bar=False,\n",
    "        )\n",
    "        trainer.fit(model, train_dataloaders=dm.train_dataloader())\n",
    "        metric = _evaluate_on_test(model, dm)     # AUROC (大きいほど良)\n",
    "        return metric\n",
    "\n",
    "    else:                          # val で評価 → validation ループ使用\n",
    "        trainer = Trainer(\n",
    "            logger=False,\n",
    "            default_root_dir=TEMP_DIR / \"optuna\",\n",
    "            accelerator=\"gpu\" if GPU_OK else \"cpu\",\n",
    "            max_epochs=5,\n",
    "            enable_progress_bar=True,\n",
    "        )\n",
    "        trainer.fit(\n",
    "            model,\n",
    "            train_dataloaders = dm.train_dataloader(),\n",
    "            val_dataloaders   = dm.val_dataloader(),\n",
    "        )\n",
    "        mse = _evaluate_on_val(model, dm)         # MSE (小さいほど良)\n",
    "        print(f\"[VAL] mean-MSE={mse:.6f}\")\n",
    "        return -mse                               # maximize(−MSE) ≡ minimize(MSE)\n",
    "\n",
    "# -------- 探索実行 --------\n",
    "TEMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=False)\n",
    "\n",
    "print(\"BEST :\", study.best_params)\n",
    "best_metric = study.best_value if EVAL_WITH_TEST_ANOM else -study.best_value\n",
    "print(\"Best AUROC:\", study.best_value)\n",
    "\n",
    "# -------- 結果保存 --------\n",
    "PARAM_DIR.mkdir(exist_ok=True)\n",
    "yaml.safe_dump(\n",
    "    dict(search_space=dict(latent_dim=LATENT_OPTS, lr=LR_RANGE,\n",
    "                           eval_with_test_anom=EVAL_WITH_TEST_ANOM),\n",
    "        ),\n",
    "    open(PARAM_DIR / \"search_space.yaml\", \"w\")\n",
    ")\n",
    "yaml.safe_dump(study.best_params, open(PARAM_DIR / \"best_params.yaml\", \"w\"))\n",
    "study.trials_dataframe().to_csv(PARAM_DIR / \"trials.csv\", index=False)\n",
    "\n",
    "# ---------- TEMP_DIR のクリーンアップ ----------\n",
    "# Debugする場合は、コメントアウトしてください\n",
    "import shutil, gc\n",
    "if TEMP_DIR.exists():\n",
    "    # Windows でハンドルが残ると削除に失敗することがあるので、念のため GC\n",
    "    gc.collect()\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73415335",
   "metadata": {},
   "source": [
    "## 3. 学習\n",
    "\n",
    "- 「2. ハイパーパラメータ探索(Optuna)」 を行った場合は、保存されたベストパラメータで学習します\n",
    "- 自動探索を行わない行場合は、ハイパーパラメータ 「MANUAL_PARAMS」を手動で調整してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1891f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import numpy as np, torch, yaml\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "# -------- GPU利用可否 --------\n",
    "GPU_OK = torch.cuda.is_available()\n",
    "\n",
    "# -------- 学習設定 --------\n",
    "MAX_EPOCHS = 1            # VAE はエポックを回すのため可変(★変更対象)\n",
    "\n",
    "# -------- ★手動パラメータ（best_params.yaml が無い時に使用） --------\n",
    "MANUAL_PARAMS = dict(\n",
    "    latent_dim    = 128,\n",
    "    learning_rate = 1e-3,\n",
    ")\n",
    "\n",
    "# -------- best_params.yaml 読み込み --------\n",
    "best_params_path = PARAM_DIR / \"best_params.yaml\"\n",
    "\n",
    "if best_params_path.exists():\n",
    "    cfg = yaml.safe_load(open(best_params_path))\n",
    "    print(\"▶ Using best_params.yaml:\", cfg)\n",
    "else:\n",
    "    cfg = MANUAL_PARAMS.copy()\n",
    "    print(\"▶ Using manual params:\", cfg)\n",
    "\n",
    "# -------- Model & DataModule --------\n",
    "model = ConvVAE(\n",
    "    latent_dim = cfg[\"latent_dim\"],\n",
    "    lr         = cfg.get(\"learning_rate\", 1e-3),\n",
    "    img_size   = IMAGE_SIZE,\n",
    ")\n",
    "\n",
    "dm = build_datamodule(batch_size=32)\n",
    "dm.setup()\n",
    "\n",
    "# -------- Lightning学習 (GPU/CPU フォールバック) --------\n",
    "tb_logger = TensorBoardLogger(save_dir=RUN_DIR / \"logs\", name=\"final\")\n",
    "\n",
    "used_gpu = None\n",
    "for use_gpu in (GPU_OK, False):\n",
    "    try:\n",
    "        trainer = Trainer(\n",
    "            default_root_dir    = TEMP_DIR / \"train\",\n",
    "            accelerator         = \"gpu\" if use_gpu else \"cpu\",\n",
    "            max_epochs          = MAX_EPOCHS,\n",
    "            log_every_n_steps   = 1,\n",
    "            enable_progress_bar = False,\n",
    "            logger              = tb_logger,\n",
    "            callbacks           = [LearningRateMonitor(logging_interval=\"step\")],\n",
    "        )\n",
    "        # ← datamodule を渡さず DataLoader を直接渡すと\n",
    "        #    Folder に起きていた prepare_data 判定バグを回避できる\n",
    "        trainer.fit(\n",
    "            model,\n",
    "            train_dataloaders = dm.train_dataloader(),\n",
    "            val_dataloaders   = dm.val_dataloader(),\n",
    "        )\n",
    "        used_gpu = use_gpu\n",
    "        break\n",
    "    except RuntimeError as e:\n",
    "        if \"cudaGetDeviceCount\" in str(e):\n",
    "            print(\"⚠️ CUDA 初期化エラー → CPU でリトライ\")\n",
    "            continue\n",
    "        raise\n",
    "\n",
    "if not used_gpu:\n",
    "    print(\"⚠️ GPU 使用不可 → CPU で学習完了\")\n",
    "\n",
    "# -------- Checkpoint 保存 --------\n",
    "ckpt_path = RUN_DIR / \"checkpoint\" / \"best.ckpt\"\n",
    "trainer.save_checkpoint(ckpt_path)\n",
    "print(\"✓ Checkpoint :\", ckpt_path)\n",
    "\n",
    "# -------- 推論用モデル .pth 保存 -------- \n",
    "state_path = RUN_DIR / \"pytorch\" / \"model.pth\"\n",
    "torch.save(model.state_dict(), state_path)\n",
    "print(\"✓ Weights :\", state_path)\n",
    "\n",
    "# ===== raw reconstruction error min/max 計算 =====\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Computing image_min and image_max ...\")\n",
    "model.eval()\n",
    "scores = []\n",
    "for batch in dm.train_dataloader():\n",
    "    x = model._prepare_x(batch).to(model.device)  # shape (B,3,IMAGE_SIZE,IMAGE_SIZE)\n",
    "    x_hat, _, _ = model(x)\n",
    "    errs = F.mse_loss(x_hat, x, reduction=\"none\").mean(dim=[1,2,3])\n",
    "    scores.extend(errs.detach().cpu().tolist())\n",
    "\n",
    "image_min = float(min(scores))\n",
    "image_max = float(max(scores))\n",
    "print(f\"Computed image_min: {image_min}, image_max: {image_max}\")\n",
    "\n",
    "# ------- .pth に含まれないメタ情報を出力(json) -------\n",
    "import json\n",
    "\n",
    "meta_path = RUN_DIR / \"pytorch\" / \"meta.json\"\n",
    "meta = {\n",
    "    \"latent_dim\"    : cfg[\"latent_dim\"],\n",
    "    \"learning_rate\" : cfg.get(\"learning_rate\", 1e-3),\n",
    "    \"image_size\"    : IMAGE_SIZE,\n",
    "    \"weights_pth\" : state_path.name,\n",
    "    \"image_threshold\"     : IMAGE_THRESHOLD,  # 手動設定\n",
    "    \"image_threshold_auto\": IMAGE_THRESHOLD,  # Test後に上書き用(任意)\n",
    "    \"raw_image_min\"       : image_min,\n",
    "    \"raw_image_max\"       : image_max,    \n",
    "    # 再構成誤差しきい値 (ConvVAEクラスに recon_threshold 等を持たせている場合)\n",
    "    #\"recon_threshold\": float(getattr(model, \"recon_threshold\", 0.0)),    \n",
    "}\n",
    "\n",
    "with open(meta_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✓ Meta JSON :\", meta_path)\n",
    "\n",
    "# ---------- TEMP_DIR のクリーンアップ ----------\n",
    "# Debugする場合は、コメントアウトしてください。\n",
    "import shutil, gc\n",
    "if TEMP_DIR.exists():\n",
    "    # Windows でハンドルが残ると削除に失敗することがあるので、念のため GC\n",
    "    gc.collect()\n",
    "    shutil.rmtree(TEMP_DIR, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebafd68-af71-4cf4-bc00-d564fb742559",
   "metadata": {},
   "source": [
    "## 4. 検出性能テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fffb21-99c0-4c1b-90ad-45a649fff250",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml, torch, pandas as pd, shutil\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms.functional as Ftv\n",
    "from torchvision.utils import make_grid\n",
    "import cv2\n",
    "\n",
    "# ---------- パス・デバイス ----------\n",
    "state_path      = Path(RUN_DIR / \"pytorch\" / \"model.pth\")\n",
    "best_param_path = PARAM_DIR / \"best_params.yaml\"\n",
    "result_root     = RUN_DIR / \"test_result\"\n",
    "device          = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ---------- latent_dim 取得 --------------------------\n",
    "# 1) cfg[\"latent_dim\"]  2) best_params.yaml  3) .pth から推定\n",
    "if cfg.get(\"latent_dim\") is not None:\n",
    "    latent_dim = int(cfg[\"latent_dim\"])\n",
    "    src_info   = \"cfg['latent_dim']\"\n",
    "\n",
    "elif best_param_path.exists():\n",
    "    _bp        = yaml.safe_load(open(best_param_path))\n",
    "    latent_dim = int(_bp[\"latent_dim\"])\n",
    "    src_info   = \"best_params.yaml\"\n",
    "\n",
    "else:\n",
    "    sd = torch.load(state_path, map_location=\"cpu\")\n",
    "    for k in (\"mu.weight\", \"encoder.mu.weight\", \"mu.linear.weight\"):\n",
    "        if k in sd:\n",
    "            latent_dim = sd[k].shape[0]          # out_features\n",
    "            src_info   = f\"model.pth ({k})\"\n",
    "            break\n",
    "    else:\n",
    "        raise RuntimeError(\"❌ latent_dim を特定できません\")\n",
    "\n",
    "print(f\"▶ latent_dim = {latent_dim}  (from {src_info})\")\n",
    "\n",
    "# ---------- モデル ----------\n",
    "model = ConvVAE(latent_dim=latent_dim).to(device)\n",
    "model.load_state_dict(torch.load(state_path, map_location=device), strict=True)\n",
    "model.eval()\n",
    "\n",
    "# ---------- DataModule ----------\n",
    "dm = build_datamodule(); dm.setup(\"test\")\n",
    "\n",
    "img_dir = result_root / \"images\"     \n",
    "(img_dir / \"normal\").mkdir(parents=True, exist_ok=True)\n",
    "(img_dir / \"anomaly\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- テスト全体の min / max を取る -----------------\n",
    "all_maps = []\n",
    "with torch.no_grad():\n",
    "    for batch in dm.test_dataloader():\n",
    "        x, _ = extract_xy(batch, device)\n",
    "        x  = model._prepare_x(x)\n",
    "        xh, _, _ = model(x)\n",
    "        diff = (x - xh).abs().mean(1, keepdim=True)   # (B,1,H,W)\n",
    "        all_maps.append(diff.cpu())\n",
    "\n",
    "err_stack = torch.cat(all_maps, dim=0)     # (N,1,H,W)\n",
    "g_min, g_max = err_stack.min(), err_stack.max()\n",
    "rng = (g_max - g_min).clamp_min(1e-6)\n",
    "\n",
    "errs, labels, paths = [], [], []\n",
    "\n",
    "# ---------- 再度回して PNG を作る --------------------------\n",
    "alpha = 0.5                     # Overlay 係数\n",
    "with torch.no_grad():\n",
    "    for batch in dm.test_dataloader():\n",
    "        x, y, pths = extract_xy(batch, device, return_paths=True)\n",
    "        x  = model._prepare_x(x)\n",
    "        xh, _, _  = model(x)\n",
    "        diff = (x - xh).abs().mean(1, keepdim=True)   # (B,1,H,W)\n",
    "\n",
    "        for img_in, img_out, dmap, fname, gt in zip(\n",
    "                x.cpu(), xh.cpu(), diff.cpu(), pths, y.int()):\n",
    "\n",
    "            # ------ 0-1 正規化（テスト全体で共通） ----------\n",
    "            d_norm = ((dmap - g_min) / rng).clamp(0, 1)          # (1,H,W)\n",
    "\n",
    "            # ------ Gaussian Blur (5×5) ----------\n",
    "            d_np = d_norm.squeeze(0).numpy()\n",
    "            d_np = cv2.GaussianBlur(d_np, (5, 5), 0)\n",
    "\n",
    "            # ------ Jet カラーマップ → Tensor(3,H,W) ----------\n",
    "            cmap = cv2.applyColorMap((d_np * 255).astype(np.uint8),\n",
    "                                     cv2.COLORMAP_JET)           # BGR, H,W,3\n",
    "            cmap = torch.from_numpy(cmap[:, :, ::-1].copy())      # →RGB\n",
    "            cmap = cmap.permute(2, 0, 1).float() / 255.0          # 3,H,W\n",
    "\n",
    "            # ------ α Blending ----------\n",
    "            overlay = alpha * cmap + (1 - alpha) * img_in.cpu()\n",
    "\n",
    "            # ------ 横連結：Input | Overlay | Recon ----------\n",
    "            trio = make_grid([img_in.cpu(), overlay, img_out.cpu()], nrow=3)\n",
    "\n",
    "            subdir = \"anomaly\" if gt.item() == 1 else \"normal\"\n",
    "            save_image(trio, img_dir / subdir / Path(fname).name) \n",
    "\n",
    "        # 指標用バッファ \n",
    "        errs  .append(F.mse_loss(xh, x, reduction=\"none\").mean([1, 2, 3]).cpu())\n",
    "        labels.append(y.cpu())\n",
    "        paths.extend([Path(p).name for p in pths])\n",
    "\n",
    "errs   = torch.cat(errs)\n",
    "labels = torch.cat(labels)\n",
    "auroc  = roc_auc_score(labels.numpy(), errs.numpy())\n",
    "f1     = f1_score(labels.numpy(), (errs > errs.mean()).numpy())  # 簡易 F1\n",
    "\n",
    "# testで自動決定される image_threshold \n",
    "threshold = errs.mean().item()\n",
    "print(f\"\\n── image_threshold used: {threshold:.4f}\\n\")\n",
    "# ★ meta.json に反映して TensorRT側へ連携することも可能 (今回はなし)\n",
    "\n",
    "# ---------- CSV 生成 ----------\n",
    "records = []\n",
    "for file, score, lab in zip(paths, errs.numpy(), labels.numpy()):\n",
    "    pred = \"anomaly\" if score > threshold else \"normal\"\n",
    "    gt   = \"anomaly\" if lab == 1 else \"normal\"\n",
    "    print(f\"{file:40s} | score={score:7.4f} | pred={pred:7s} | label={gt}\")\n",
    "    records.append(dict(file=file, score=float(score), pred=pred, label=gt))\n",
    "\n",
    "# ---------- CSV 保存 ----------\n",
    "result_root.mkdir(parents=True, exist_ok=True)\n",
    "csv_path = result_root / \"predictions.csv\"\n",
    "pd.DataFrame(records).to_csv(csv_path, index=False)\n",
    "print(f\"\\n✓ predictions.csv saved to {csv_path}\\n\")\n",
    "\n",
    "# ---------- meta.json 更新 (image_threshold_auto) ----------\n",
    "meta_path = Path(RUN_DIR) / \"pytorch\" / \"meta.json\"\n",
    "with open(meta_path, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "meta[\"image_threshold_auto\"] = float(threshold)\n",
    "with open(meta_path, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✓ threshold_auto updated to {meta_path}\")\n",
    "\n",
    "# ---------- 結果表示 ----------\n",
    "print(\"\\n==========  EVALUATION  ==========\")\n",
    "print(f\"Images tested : {len(dm.test_data)}\")\n",
    "print(f\"AUROC         : {auroc:7.4f}\")\n",
    "print(f\"Best F1       : {f1:7.4f}\")\n",
    "print(\"===================================\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23064766-0459-44d4-ab78-b79612da6bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
